{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install google-cloud-texttospeech"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 499
        },
        "id": "3MKVSycZ5hIq",
        "outputId": "696672f4-12b7-4518-c606-02451da53234"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting google-cloud-texttospeech\n",
            "  Downloading google_cloud_texttospeech-2.16.3-py2.py3-none-any.whl (151 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/152.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m152.0/152.0 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1 in /usr/local/lib/python3.10/dist-packages (from google-cloud-texttospeech) (2.11.1)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1 in /usr/local/lib/python3.10/dist-packages (from google-cloud-texttospeech) (2.27.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from google-cloud-texttospeech) (1.23.0)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5 in /usr/local/lib/python3.10/dist-packages (from google-cloud-texttospeech) (3.20.3)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-texttospeech) (1.63.1)\n",
            "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-texttospeech) (2.31.0)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-texttospeech) (1.64.1)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-texttospeech) (1.48.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-cloud-texttospeech) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-cloud-texttospeech) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-cloud-texttospeech) (4.9)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-cloud-texttospeech) (0.6.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-texttospeech) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-texttospeech) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-texttospeech) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-texttospeech) (2024.6.2)\n",
            "Installing collected packages: google-cloud-texttospeech\n",
            "Successfully installed google-cloud-texttospeech-2.16.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              },
              "id": "403ace88ca75467bac40a7e44edb3483"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pydub"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tn21WpO15lqX",
        "outputId": "3183a780-3934-45f4-c884-f307bb74479e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pydub\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Installing collected packages: pydub\n",
            "Successfully installed pydub-0.25.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E32EQZ2FXXES",
        "outputId": "2febf1e0-5d0f-4197-f7ba-da732b00cafb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "jCVp7SUCXYvR"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"/content/drive/MyDrive/Bangkit/wira-wicara-d40ed01294ac.json\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vHPpeGQ2XaQy",
        "outputId": "d6236698-7308-400b-e209-f582324926b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated: tts_output/kucing_menangis/id-ID-Standard-A_pitch-2.0_rate0.8_0.wav\n",
            "Generated: tts_output/kucing_menangis/id-ID-Standard-A_pitch-1.7_rate0.8300000000000001_1.wav\n",
            "Generated: tts_output/kucing_menangis/id-ID-Standard-A_pitch-1.4_rate0.8600000000000001_2.wav\n",
            "Generated: tts_output/kucing_menangis/id-ID-Standard-A_pitch-1.1_rate0.89_3.wav\n",
            "Generated: tts_output/kucing_menangis/id-ID-Standard-B_pitch-2.0_rate0.8_0.wav\n",
            "Generated: tts_output/kucing_menangis/id-ID-Standard-B_pitch-1.7_rate0.8300000000000001_1.wav\n",
            "Generated: tts_output/kucing_menangis/id-ID-Standard-B_pitch-1.4_rate0.8600000000000001_2.wav\n",
            "Generated: tts_output/kucing_menangis/id-ID-Standard-B_pitch-1.1_rate0.89_3.wav\n",
            "Generated: tts_output/kucing_menangis/id-ID-Standard-C_pitch-2.0_rate0.8_0.wav\n",
            "Generated: tts_output/kucing_menangis/id-ID-Standard-C_pitch-1.7_rate0.8300000000000001_1.wav\n",
            "Generated: tts_output/kucing_menangis/id-ID-Standard-C_pitch-1.4_rate0.8600000000000001_2.wav\n",
            "Generated: tts_output/kucing_menangis/id-ID-Standard-C_pitch-1.1_rate0.89_3.wav\n",
            "Generated: tts_output/kucing_menangis/id-ID-Standard-D_pitch-2.0_rate0.8_0.wav\n",
            "Generated: tts_output/kucing_menangis/id-ID-Standard-D_pitch-1.7_rate0.8300000000000001_1.wav\n",
            "Generated: tts_output/kucing_menangis/id-ID-Standard-D_pitch-1.4_rate0.8600000000000001_2.wav\n",
            "Generated: tts_output/kucing_menangis/id-ID-Standard-D_pitch-1.1_rate0.89_3.wav\n",
            "Generated: tts_output/ibu_melempar_batu/id-ID-Standard-A_pitch-2.0_rate0.8_0.wav\n",
            "Generated: tts_output/ibu_melempar_batu/id-ID-Standard-A_pitch-1.7_rate0.8300000000000001_1.wav\n",
            "Generated: tts_output/ibu_melempar_batu/id-ID-Standard-A_pitch-1.4_rate0.8600000000000001_2.wav\n",
            "Generated: tts_output/ibu_melempar_batu/id-ID-Standard-A_pitch-1.1_rate0.89_3.wav\n",
            "Generated: tts_output/ibu_melempar_batu/id-ID-Standard-B_pitch-2.0_rate0.8_0.wav\n",
            "Generated: tts_output/ibu_melempar_batu/id-ID-Standard-B_pitch-1.7_rate0.8300000000000001_1.wav\n",
            "Generated: tts_output/ibu_melempar_batu/id-ID-Standard-B_pitch-1.4_rate0.8600000000000001_2.wav\n",
            "Generated: tts_output/ibu_melempar_batu/id-ID-Standard-B_pitch-1.1_rate0.89_3.wav\n",
            "Generated: tts_output/ibu_melempar_batu/id-ID-Standard-C_pitch-2.0_rate0.8_0.wav\n",
            "Generated: tts_output/ibu_melempar_batu/id-ID-Standard-C_pitch-1.7_rate0.8300000000000001_1.wav\n",
            "Generated: tts_output/ibu_melempar_batu/id-ID-Standard-C_pitch-1.4_rate0.8600000000000001_2.wav\n",
            "Generated: tts_output/ibu_melempar_batu/id-ID-Standard-C_pitch-1.1_rate0.89_3.wav\n",
            "Generated: tts_output/ibu_melempar_batu/id-ID-Standard-D_pitch-2.0_rate0.8_0.wav\n",
            "Generated: tts_output/ibu_melempar_batu/id-ID-Standard-D_pitch-1.7_rate0.8300000000000001_1.wav\n",
            "Generated: tts_output/ibu_melempar_batu/id-ID-Standard-D_pitch-1.4_rate0.8600000000000001_2.wav\n",
            "Generated: tts_output/ibu_melempar_batu/id-ID-Standard-D_pitch-1.1_rate0.89_3.wav\n",
            "Generated: tts_output/apel_newton_jatuh/id-ID-Standard-A_pitch-2.0_rate0.8_0.wav\n",
            "Generated: tts_output/apel_newton_jatuh/id-ID-Standard-A_pitch-1.7_rate0.8300000000000001_1.wav\n",
            "Generated: tts_output/apel_newton_jatuh/id-ID-Standard-A_pitch-1.4_rate0.8600000000000001_2.wav\n",
            "Generated: tts_output/apel_newton_jatuh/id-ID-Standard-A_pitch-1.1_rate0.89_3.wav\n",
            "Generated: tts_output/apel_newton_jatuh/id-ID-Standard-B_pitch-2.0_rate0.8_0.wav\n",
            "Generated: tts_output/apel_newton_jatuh/id-ID-Standard-B_pitch-1.7_rate0.8300000000000001_1.wav\n",
            "Generated: tts_output/apel_newton_jatuh/id-ID-Standard-B_pitch-1.4_rate0.8600000000000001_2.wav\n",
            "Generated: tts_output/apel_newton_jatuh/id-ID-Standard-B_pitch-1.1_rate0.89_3.wav\n",
            "Generated: tts_output/apel_newton_jatuh/id-ID-Standard-C_pitch-2.0_rate0.8_0.wav\n",
            "Generated: tts_output/apel_newton_jatuh/id-ID-Standard-C_pitch-1.7_rate0.8300000000000001_1.wav\n",
            "Generated: tts_output/apel_newton_jatuh/id-ID-Standard-C_pitch-1.4_rate0.8600000000000001_2.wav\n",
            "Generated: tts_output/apel_newton_jatuh/id-ID-Standard-C_pitch-1.1_rate0.89_3.wav\n",
            "Generated: tts_output/apel_newton_jatuh/id-ID-Standard-D_pitch-2.0_rate0.8_0.wav\n",
            "Generated: tts_output/apel_newton_jatuh/id-ID-Standard-D_pitch-1.7_rate0.8300000000000001_1.wav\n",
            "Generated: tts_output/apel_newton_jatuh/id-ID-Standard-D_pitch-1.4_rate0.8600000000000001_2.wav\n",
            "Generated: tts_output/apel_newton_jatuh/id-ID-Standard-D_pitch-1.1_rate0.89_3.wav\n",
            "Dataset generation complete.\n"
          ]
        }
      ],
      "source": [
        "from google.cloud import texttospeech\n",
        "import os\n",
        "\n",
        "# Set up the client\n",
        "client = texttospeech.TextToSpeechClient()\n",
        "\n",
        "# Sentences to be converted to speech\n",
        "sentences = [\n",
        "    \"kucing menangis\",\n",
        "    \"ibu melempar batu\",\n",
        "    \"apel newton jatuh\"\n",
        "]\n",
        "\n",
        "# Voice parameters\n",
        "voices = [\n",
        "    {\"language_code\": \"id-ID\", \"name\": \"id-ID-Standard-A\", \"ssml_gender\": texttospeech.SsmlVoiceGender.FEMALE},\n",
        "    {\"language_code\": \"id-ID\", \"name\": \"id-ID-Standard-B\", \"ssml_gender\": texttospeech.SsmlVoiceGender.MALE},\n",
        "    {\"language_code\": \"id-ID\", \"name\": \"id-ID-Standard-C\", \"ssml_gender\": texttospeech.SsmlVoiceGender.MALE},\n",
        "    {\"language_code\": \"id-ID\", \"name\": \"id-ID-Standard-D\", \"ssml_gender\": texttospeech.SsmlVoiceGender.FEMALE}\n",
        "]\n",
        "\n",
        "# Create directory to store audio files\n",
        "base_output_dir = \"tts_output\"\n",
        "os.makedirs(base_output_dir, exist_ok=True)\n",
        "\n",
        "# Function to generate audio files\n",
        "def generate_audio(sentence, voice, pitch, rate, index):\n",
        "    text_input = texttospeech.SynthesisInput(text=sentence)\n",
        "    audio_config = texttospeech.AudioConfig(\n",
        "        audio_encoding=texttospeech.AudioEncoding.LINEAR16,\n",
        "        sample_rate_hertz=8000,\n",
        "        pitch=pitch,\n",
        "        speaking_rate=rate\n",
        "    )\n",
        "\n",
        "    response = client.synthesize_speech(\n",
        "        input=text_input,\n",
        "        voice=voice,\n",
        "        audio_config=audio_config\n",
        "    )\n",
        "\n",
        "    # Create a directory for each sentence\n",
        "    sentence_dir = os.path.join(base_output_dir, sentence.replace(\" \", \"_\"))\n",
        "    os.makedirs(sentence_dir, exist_ok=True)\n",
        "\n",
        "    # Save the audio file in the respective sentence directory\n",
        "    filename = f\"{sentence_dir}/{voice['name']}_pitch{pitch}_rate{rate}_{index}.wav\"\n",
        "    with open(filename, \"wb\") as out:\n",
        "        out.write(response.audio_content)\n",
        "    print(f\"Generated: {filename}\")\n",
        "\n",
        "# Generate 50 files for each sentence with different pitch and speaking rate values\n",
        "for sentence in sentences:\n",
        "    for voice in voices:\n",
        "        for i in range(4):\n",
        "            pitch = -2.0 + (i * 0.3)\n",
        "            rate = 0.8 + (i * 0.03)\n",
        "            generate_audio(sentence, voice, pitch, rate, i)\n",
        "\n",
        "print(\"Dataset generation complete.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "n_J2MWNtXcKG"
      },
      "outputs": [],
      "source": [
        "from pydub import AudioSegment\n",
        "from pydub.generators import WhiteNoise\n",
        "import os\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ic62TC2fXe70"
      },
      "outputs": [],
      "source": [
        "def normalize_audio(audio, target_dBFS=-20.0, target_sample_rate=8000):\n",
        "    change_in_dBFS = target_dBFS - audio.dBFS\n",
        "    return audio.apply_gain(change_in_dBFS).set_frame_rate(target_sample_rate)\n",
        "\n",
        "def add_white_noise(audio, noise_level=0.005):\n",
        "    noise = WhiteNoise().to_audio_segment(duration=len(audio))\n",
        "    noise = noise - (noise.dBFS - audio.dBFS) + 2  # Adjust noise to desired level\n",
        "    return audio.overlay(noise - noise_level)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WNR14Z5LXgkS",
        "outputId": "32a045fb-dc05-4ee4-dcaf-2bdca68adcce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Normalization complete.\n"
          ]
        }
      ],
      "source": [
        "# Define input and output directories\n",
        "base_output_dir = \"tts_output\"\n",
        "normalized_output_dir = \"normalized\"\n",
        "normalized_augmented_output_dir = \"normalized_augmented\"\n",
        "fused_output_dir = \"fused\"\n",
        "\n",
        "os.makedirs(normalized_output_dir, exist_ok=True)\n",
        "os.makedirs(normalized_augmented_output_dir, exist_ok=True)\n",
        "os.makedirs(fused_output_dir, exist_ok=True)\n",
        "\n",
        "for sentence_folder in os.listdir(base_output_dir):\n",
        "    sentence_folder_path = os.path.join(base_output_dir, sentence_folder)\n",
        "    if os.path.isdir(sentence_folder_path):\n",
        "        # Create corresponding folders in the output directories\n",
        "        normalized_sentence_folder_path = os.path.join(normalized_output_dir, sentence_folder)\n",
        "        os.makedirs(normalized_sentence_folder_path, exist_ok=True)\n",
        "\n",
        "        augmented_sentence_folder_path = os.path.join(normalized_augmented_output_dir, sentence_folder)\n",
        "        os.makedirs(augmented_sentence_folder_path, exist_ok=True)\n",
        "\n",
        "        fused_sentence_folder_path = os.path.join(fused_output_dir, sentence_folder)\n",
        "        os.makedirs(fused_sentence_folder_path, exist_ok=True)\n",
        "\n",
        "        # Collect all audio files in the current sentence folder\n",
        "        audio_files = [f for f in os.listdir(sentence_folder_path) if f.endswith(\".wav\")]\n",
        "\n",
        "        # Normalize each audio file and save to 'normalized' folder\n",
        "        for filename in audio_files:\n",
        "            audio_path = os.path.join(sentence_folder_path, filename)\n",
        "            original_audio = AudioSegment.from_file(audio_path)\n",
        "\n",
        "            # Normalize audio\n",
        "            normalized_audio = normalize_audio(original_audio)\n",
        "\n",
        "            # Save normalized audio to 'normalized' folder\n",
        "            normalized_output_path = os.path.join(normalized_sentence_folder_path, filename)\n",
        "            normalized_audio.export(normalized_output_path, format=\"wav\")\n",
        "\n",
        "print(\"Normalization complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HHJsnvJZTO1S"
      },
      "source": [
        "### ML Model Process for Level 2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N0mByhyNTO1W"
      },
      "source": [
        "#### Import Library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Zyy69Y3xTO1X"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Conv1D, Flatten, Dense, Lambda, Dropout, MaxPooling1D\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from sklearn.model_selection import train_test_split\n",
        "import librosa\n",
        "import pydub"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k5vqWf0qTO1Z"
      },
      "source": [
        "#### Audio Data Processing Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "JT7lH8-JTO1e"
      },
      "outputs": [],
      "source": [
        "# Function to load and preprocess audio files\n",
        "def load_and_preprocess(file_path, target_length=16000):\n",
        "    audio, _ = librosa.load(file_path, sr=16000, mono=True)\n",
        "\n",
        "    # Ensure audio length is not greater than target_length\n",
        "    if len(audio) > target_length:\n",
        "        audio = audio[:target_length]\n",
        "    else:\n",
        "        # Pad audio to target_length if shorter\n",
        "        pad_amount = target_length - len(audio)\n",
        "        audio = np.pad(audio, (0, pad_amount), mode='constant')\n",
        "\n",
        "    # Normalize audio\n",
        "    audio = audio / np.max(np.abs(audio))\n",
        "\n",
        "    # Reshape audio to include time steps dimension\n",
        "    audio = np.expand_dims(audio, axis=-1)\n",
        "\n",
        "    return audio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "KTSdCJn-TO1f"
      },
      "outputs": [],
      "source": [
        "def convert_and_normalize(input_path, output_path, target_dBFS=-20.0, target_sample_rate=8000):\n",
        "    # Load audio using pydub (support for various formats including mp3, wav etc)\n",
        "    audio = pydub.AudioSegment.from_file(input_path)\n",
        "\n",
        "    # Normalize audio to target dBFS and sample rate\n",
        "    change_in_dBFS = target_dBFS - audio.dBFS\n",
        "    normalized_audio = audio.apply_gain(change_in_dBFS)\n",
        "    normalized_audio = normalized_audio.set_frame_rate(target_sample_rate)\n",
        "\n",
        "    # Export normalized audio to WAV format\n",
        "    normalized_audio.export(output_path, format=\"wav\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "GfVzJ4C1TO1h"
      },
      "outputs": [],
      "source": [
        "# Function to load augmented dataset\n",
        "def load_data(base_dir):\n",
        "    sentences = []\n",
        "    file_paths = []\n",
        "\n",
        "    for sentence in os.listdir(base_dir):\n",
        "        sentence_dir = os.path.join(base_dir, sentence)\n",
        "        for file in os.listdir(sentence_dir):\n",
        "            if file.endswith(\".wav\"):\n",
        "                file_paths.append(os.path.join(sentence_dir, file))\n",
        "                sentences.append(sentence)\n",
        "\n",
        "    return np.array(file_paths), np.array(sentences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "jsZ6cAIXTO1h"
      },
      "outputs": [],
      "source": [
        "# Function to create pairs of audio samples with their labels\n",
        "def create_pairs(files, sentences):\n",
        "    pairs = []\n",
        "    labels = []\n",
        "    num_samples = len(files)\n",
        "\n",
        "    for i in range(num_samples):\n",
        "        for j in range(i+1, num_samples):\n",
        "            if sentences[i] == sentences[j]:\n",
        "                pairs.append((i, j))\n",
        "                labels.append(1)\n",
        "            else:\n",
        "                pairs.append((i, j))\n",
        "                labels.append(0)\n",
        "\n",
        "    return np.array(pairs, dtype=np.int32), np.array(labels, dtype=np.int32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GYOKb087TO1i"
      },
      "source": [
        "#### Data Generator for Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Zaf9s2kITO1i"
      },
      "outputs": [],
      "source": [
        "def data_generator(files, sentences, batch_size=32, target_length=16000):\n",
        "    while True:\n",
        "        indices = np.random.permutation(len(files))\n",
        "        pairs, labels = create_pairs(files, sentences)\n",
        "        batch_start = 0\n",
        "        while batch_start < len(pairs):\n",
        "            batch_end = min(batch_start + batch_size, len(pairs))\n",
        "            batch_indices = indices[batch_start:batch_end]\n",
        "            batch_pairs = pairs[batch_indices]\n",
        "            batch_labels = labels[batch_indices]\n",
        "\n",
        "            audio_1 = np.array([load_and_preprocess(files[i], target_length) for i in batch_pairs[:, 0]])\n",
        "            audio_2 = np.array([load_and_preprocess(files[i], target_length) for i in batch_pairs[:, 1]])\n",
        "\n",
        "            # Check if batch_pairs is empty\n",
        "            if len(batch_pairs) == 0:\n",
        "                break\n",
        "\n",
        "            # Yield batches\n",
        "            yield [audio_1, audio_2], batch_labels\n",
        "            batch_start += batch_size\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y91matJYTO1i"
      },
      "source": [
        "#### Define Siamese CNN Model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, Dropout, Flatten, Dense, Lambda, BatchNormalization, LeakyReLU\n",
        "from tensorflow.keras.models import Model\n",
        "import tensorflow.keras.backend as K\n",
        "\n",
        "def create_siamese_model(input_shape):\n",
        "    def cnn_network(input_shape):\n",
        "        model = tf.keras.Sequential()\n",
        "\n",
        "        # First Convolutional Layer\n",
        "        model.add(Conv1D(64, 7, padding='same', input_shape=input_shape))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(LeakyReLU(alpha=0.1))\n",
        "        model.add(MaxPooling1D(pool_size=2))\n",
        "        model.add(Dropout(0.3))\n",
        "\n",
        "        # Second Convolutional Layer\n",
        "        model.add(Conv1D(128, 5, padding='same'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(LeakyReLU(alpha=0.1))\n",
        "        model.add(MaxPooling1D(pool_size=2))\n",
        "        model.add(Dropout(0.3))\n",
        "\n",
        "        # Third Convolutional Layer\n",
        "        model.add(Conv1D(128, 5, padding='same'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(LeakyReLU(alpha=0.1))\n",
        "        model.add(MaxPooling1D(pool_size=2))\n",
        "        model.add(Dropout(0.3))\n",
        "\n",
        "        # Third Convolutional Layer\n",
        "        model.add(Conv1D(128, 3, padding='same'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(LeakyReLU(alpha=0.1))\n",
        "        model.add(MaxPooling1D(pool_size=2))\n",
        "        model.add(Dropout(0.3))\n",
        "\n",
        "        # Flatten and Dense Layers\n",
        "        model.add(Flatten())\n",
        "        model.add(Dense(256, activation='relu'))\n",
        "        model.add(Dropout(0.3))\n",
        "        model.add(Dense(256, activation='relu'))\n",
        "        model.add(Dropout(0.3))\n",
        "\n",
        "        return model\n",
        "\n",
        "    input_left = Input(shape=input_shape)\n",
        "    input_right = Input(shape=input_shape)\n",
        "\n",
        "    cnn = cnn_network(input_shape)\n",
        "\n",
        "    encoded_left = cnn(input_left)\n",
        "    encoded_right = cnn(input_right)\n",
        "\n",
        "    # L1 distance layer between the two encoded outputs\n",
        "    L1_distance = Lambda(lambda x: K.abs(x[0] - x[1]))\n",
        "    L1_distance_out = L1_distance([encoded_left, encoded_right])\n",
        "\n",
        "    # Prediction layer\n",
        "    prediction = Dense(1, activation='sigmoid')(L1_distance_out)\n",
        "\n",
        "    # Model instance\n",
        "    siamese_model = Model(inputs=[input_left, input_right], outputs=prediction)\n",
        "\n",
        "    return siamese_model\n",
        "\n",
        "# Example usage\n",
        "input_shape = (128, 1)  # Example input shape\n",
        "siamese_model = create_siamese_model(input_shape)\n",
        "siamese_model.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sAv2EFOMNNIo",
        "outputId": "6de3eafa-01fd-4aaa-d441-baccb821a8ef"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_4\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_9 (InputLayer)        [(None, 128, 1)]             0         []                            \n",
            "                                                                                                  \n",
            " input_10 (InputLayer)       [(None, 128, 1)]             0         []                            \n",
            "                                                                                                  \n",
            " sequential_4 (Sequential)   (None, 256)                  502912    ['input_9[0][0]',             \n",
            "                                                                     'input_10[0][0]']            \n",
            "                                                                                                  \n",
            " lambda_4 (Lambda)           (None, 256)                  0         ['sequential_4[0][0]',        \n",
            "                                                                     'sequential_4[1][0]']        \n",
            "                                                                                                  \n",
            " dense_13 (Dense)            (None, 1)                    257       ['lambda_4[0][0]']            \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 503169 (1.92 MB)\n",
            "Trainable params: 502273 (1.92 MB)\n",
            "Non-trainable params: 896 (3.50 KB)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "i1xAWgPFTO1i"
      },
      "outputs": [],
      "source": [
        "# Function to create the Siamese CNN model\n",
        "def create_siamese_model(input_shape):\n",
        "    def cnn_network(input_shape):\n",
        "        model = tf.keras.Sequential()\n",
        "        model.add(Conv1D(64, 5, activation='relu', input_shape=input_shape))\n",
        "        model.add(MaxPooling1D(pool_size=2))\n",
        "        model.add(Dropout(0.2))\n",
        "        model.add(Conv1D(128, 5, activation='relu'))\n",
        "        model.add(MaxPooling1D(pool_size=2))\n",
        "        model.add(Dropout(0.2))\n",
        "        model.add(Flatten())\n",
        "        model.add(Dense(256, activation='relu'))\n",
        "        model.add(Dropout(0.2))\n",
        "        return model\n",
        "\n",
        "    input_left = Input(shape=input_shape)\n",
        "    input_right = Input(shape=input_shape)\n",
        "\n",
        "    cnn = cnn_network(input_shape)\n",
        "\n",
        "    encoded_left = cnn(input_left)\n",
        "    encoded_right = cnn(input_right)\n",
        "\n",
        "    # L1 distance layer between the two encoded outputs\n",
        "    L1_distance = Lambda(lambda x: K.abs(x[0] - x[1]))\n",
        "    L1_distance_out = L1_distance([encoded_left, encoded_right])\n",
        "\n",
        "    # Prediction layer\n",
        "    prediction = Dense(1, activation='sigmoid')(L1_distance_out)\n",
        "\n",
        "    # Model instance\n",
        "    siamese_model = Model(inputs=[input_left, input_right], outputs=prediction)\n",
        "\n",
        "    return siamese_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MYld9rCTTO1j"
      },
      "source": [
        "#### Load and Split Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "nKJDMU_uTO1k"
      },
      "outputs": [],
      "source": [
        "# Directory containing augmented TTS output\n",
        "output_dir = \"/content/tts_output\"\n",
        "\n",
        "# Load augmented dataset\n",
        "file_paths, sentences = load_data(output_dir)\n",
        "\n",
        "# Split data into training and validation sets\n",
        "train_files, val_files, train_sentences, val_sentences = train_test_split(file_paths, sentences, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZUMi1CcYTO1l"
      },
      "source": [
        "#### Create Generators and Compile Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "XrbN3M9TTO1l"
      },
      "outputs": [],
      "source": [
        "# Create generators\n",
        "train_gen = data_generator(train_files, train_sentences, batch_size=16)\n",
        "val_gen = data_generator(val_files, val_sentences, batch_size=16)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YvwAWS5vTO1l",
        "outputId": "f762ebab-8042-43b7-f930-5dd9e707c37c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_5\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_11 (InputLayer)       [(None, 16000, 1)]           0         []                            \n",
            "                                                                                                  \n",
            " input_12 (InputLayer)       [(None, 16000, 1)]           0         []                            \n",
            "                                                                                                  \n",
            " sequential_5 (Sequential)   (None, 256)                  3300876   ['input_11[0][0]',            \n",
            "                                                          8          'input_12[0][0]']            \n",
            "                                                                                                  \n",
            " lambda_5 (Lambda)           (None, 256)                  0         ['sequential_5[0][0]',        \n",
            "                                                                     'sequential_5[1][0]']        \n",
            "                                                                                                  \n",
            " dense_16 (Dense)            (None, 1)                    257       ['lambda_5[0][0]']            \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 33009025 (125.92 MB)\n",
            "Trainable params: 33008129 (125.92 MB)\n",
            "Non-trainable params: 896 (3.50 KB)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# Create generators\n",
        "train_gen = data_generator(train_files, train_sentences, batch_size=32)\n",
        "val_gen = data_generator(val_files, val_sentences, batch_size=32)\n",
        "\n",
        "# Define input shape\n",
        "input_shape = (16000, 1)\n",
        "\n",
        "# Create the model\n",
        "siamese_model = create_siamese_model(input_shape)\n",
        "\n",
        "# Compile the model\n",
        "siamese_model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.0001), metrics=['accuracy'])\n",
        "siamese_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wol7aSgkTO1m"
      },
      "source": [
        "#### Train the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "GVHAk53yTO1m"
      },
      "outputs": [],
      "source": [
        "# Callbacks for training\n",
        "checkpoint = ModelCheckpoint('siamese_model.h5', monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1, restore_best_weights=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LSbHMd2uTO1m",
        "outputId": "a3e6b509-a9c4-457b-9028-ebcfda17b613"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "4/4 [==============================] - ETA: 0s - loss: 3.0343 - accuracy: 0.5263\n",
            "Epoch 1: val_loss improved from inf to 0.69230, saving model to siamese_model.h5\n",
            "4/4 [==============================] - 19s 3s/step - loss: 3.0343 - accuracy: 0.5263 - val_loss: 0.6923 - val_accuracy: 0.5000\n",
            "Epoch 2/20\n",
            "4/4 [==============================] - ETA: 0s - loss: 2.2976 - accuracy: 0.5526\n",
            "Epoch 2: val_loss did not improve from 0.69230\n",
            "4/4 [==============================] - 1s 186ms/step - loss: 2.2976 - accuracy: 0.5526 - val_loss: 0.6926 - val_accuracy: 0.7000\n",
            "Epoch 3/20\n",
            "4/4 [==============================] - ETA: 0s - loss: 2.8293 - accuracy: 0.5921\n",
            "Epoch 3: val_loss improved from 0.69230 to 0.69203, saving model to siamese_model.h5\n",
            "4/4 [==============================] - 3s 824ms/step - loss: 2.8293 - accuracy: 0.5921 - val_loss: 0.6920 - val_accuracy: 0.7000\n",
            "Epoch 4/20\n",
            "4/4 [==============================] - ETA: 0s - loss: 2.0155 - accuracy: 0.5921\n",
            "Epoch 4: val_loss did not improve from 0.69203\n",
            "4/4 [==============================] - 1s 216ms/step - loss: 2.0155 - accuracy: 0.5921 - val_loss: 0.6923 - val_accuracy: 0.5000\n",
            "Epoch 5/20\n",
            "4/4 [==============================] - ETA: 0s - loss: 2.5809 - accuracy: 0.5000\n",
            "Epoch 5: val_loss improved from 0.69203 to 0.69110, saving model to siamese_model.h5\n",
            "4/4 [==============================] - 3s 935ms/step - loss: 2.5809 - accuracy: 0.5000 - val_loss: 0.6911 - val_accuracy: 0.7000\n",
            "Epoch 6/20\n",
            "4/4 [==============================] - ETA: 0s - loss: 2.6336 - accuracy: 0.5526\n",
            "Epoch 6: val_loss did not improve from 0.69110\n",
            "4/4 [==============================] - 1s 182ms/step - loss: 2.6336 - accuracy: 0.5526 - val_loss: 0.6937 - val_accuracy: 0.4000\n",
            "Epoch 7/20\n",
            "4/4 [==============================] - ETA: 0s - loss: 2.1891 - accuracy: 0.5263\n",
            "Epoch 7: val_loss did not improve from 0.69110\n",
            "4/4 [==============================] - 1s 181ms/step - loss: 2.1891 - accuracy: 0.5263 - val_loss: 0.6919 - val_accuracy: 0.7000\n",
            "Epoch 8/20\n",
            "4/4 [==============================] - ETA: 0s - loss: 1.4020 - accuracy: 0.6711\n",
            "Epoch 8: val_loss did not improve from 0.69110\n",
            "4/4 [==============================] - 1s 185ms/step - loss: 1.4020 - accuracy: 0.6711 - val_loss: 0.6926 - val_accuracy: 0.6000\n",
            "Epoch 9/20\n",
            "4/4 [==============================] - ETA: 0s - loss: 1.9324 - accuracy: 0.5526\n",
            "Epoch 9: val_loss did not improve from 0.69110\n",
            "4/4 [==============================] - 1s 183ms/step - loss: 1.9324 - accuracy: 0.5526 - val_loss: 0.6912 - val_accuracy: 0.7000\n",
            "Epoch 10/20\n",
            "4/4 [==============================] - ETA: 0s - loss: 1.7871 - accuracy: 0.6053\n",
            "Epoch 10: val_loss did not improve from 0.69110\n",
            "Restoring model weights from the end of the best epoch: 5.\n",
            "4/4 [==============================] - 1s 252ms/step - loss: 1.7871 - accuracy: 0.6053 - val_loss: 0.6917 - val_accuracy: 0.5000\n",
            "Epoch 10: early stopping\n"
          ]
        }
      ],
      "source": [
        "# Assuming you have variables `num_train_samples` and `num_val_samples` representing\n",
        "# the total number of training and validation samples, and `batch_size` representing\n",
        "# the size of each batch\n",
        "\n",
        "steps_per_epoch = len(train_files) // 8\n",
        "validation_steps = len(val_files) // 8\n",
        "\n",
        "# Check if the calculated steps_per_epoch and validation_steps are greater than 0\n",
        "if steps_per_epoch == 0 or validation_steps == 0:\n",
        "    raise ValueError(\"Number of steps per epoch or validation steps is zero. Please check the dataset size and batch size.\")\n",
        "\n",
        "# Fit the model\n",
        "history = siamese_model.fit(\n",
        "    train_gen,\n",
        "    steps_per_epoch=steps_per_epoch,\n",
        "    validation_data=val_gen,\n",
        "    validation_steps=validation_steps,\n",
        "    epochs=20,\n",
        "    callbacks=[checkpoint, early_stopping],\n",
        "    verbose=1\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "siamese_model.save('siamese_model.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LNKlQDJu1pKK",
        "outputId": "795e152d-ae43-46cb-fc2b-da20880806f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2hMOgcs0TO1n"
      },
      "source": [
        "#### Test and Predict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "ItAZS7MgTO1n"
      },
      "outputs": [],
      "source": [
        "# Paths to the test audio files (m4a format)\n",
        "#test_audio_file_1 = 'test/ular-clear-1.m4a'\n",
        "#test_audio_file_2 = 'test/mobil-clear-1.m4a'\n",
        "\n",
        "# Paths to the converted WAV files\n",
        "#converted_audio_file_1 = 'converted_test_audio_1.wav'\n",
        "#converted_audio_file_2 = 'converted_test_audio_2.wav'\n",
        "\n",
        "# Convert the test audio files to WAV format with 8kHz sample rate\n",
        "#convert_and_normalize(test_audio_file_1, converted_audio_file_1)\n",
        "#convert_and_normalize(test_audio_file_2, converted_audio_file_2)\n",
        "\n",
        "converted_audio_file_1 = '/content/drive/MyDrive/SUARA BANGKIT/IBU MELEMPAR BATU.wav'\n",
        "converted_audio_file_2 = '/content/drive/MyDrive/SUARA BANGKIT/IBU MELEMPAR BARU 3.wav'\n",
        "\n",
        "# Load and preprocess the test audio files\n",
        "test_audio_1 = load_and_preprocess(converted_audio_file_1,target_length=16000)\n",
        "test_audio_2 = load_and_preprocess(converted_audio_file_2,target_length=16000)\n",
        "\n",
        "# Add batch dimension\n",
        "test_audio_1 = np.expand_dims(test_audio_1, axis=0)\n",
        "test_audio_2 = np.expand_dims(test_audio_2, axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "H_fNLFN8TO1n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79e9f55c-cedf-432b-9882-687cc1fb4dcd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 1s 523ms/step\n",
            "Similarity score: 99.03%\n"
          ]
        }
      ],
      "source": [
        "# Load the trained Siamese model\n",
        "#siamese_model = tf.keras.models.load_model('/content/siamese_model.h5', compile=False)\n",
        "\n",
        "# Predict the similarity\n",
        "similarity_score = siamese_model.predict([test_audio_1, test_audio_2])\n",
        "\n",
        "# Define the min and max scores for normalization\n",
        "min_score = 0.0\n",
        "max_score = 0.5\n",
        "\n",
        "# Normalize the similarity score to the range 0-100%\n",
        "normalized_similarity_score = (similarity_score[0][0] - min_score) / (max_score - min_score) * 100\n",
        "\n",
        "# Clip the value to ensure it stays within the 0-100% range\n",
        "normalized_similarity_score = np.clip(normalized_similarity_score, 0, 100)\n",
        "\n",
        "# Output the normalized similarity score\n",
        "print(f'Similarity score: {normalized_similarity_score:.2f}%')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Muat model Keras dari file .h5\n",
        "siamese_model = tf.keras.models.load_model('siamese_model.h5')\n",
        "\n",
        "# Buat konverter TFLite\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(siamese_model)\n",
        "\n",
        "# Mengatur optimasi ke kuantisasi float16\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "converter.target_spec.supported_types = [tf.float16]\n",
        "\n",
        "# Konversi model\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "# Simpan model TFLite ke file\n",
        "with open('model_quant_float16.tflite', 'wb') as f:\n",
        "    f.write(tflite_model)\n",
        "\n",
        "print(\"Model berhasil dikonversi dan disimpan sebagai model_quant_float16.tflite\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vsoBbIJOt2z8",
        "outputId": "f462197c-d8e6-4f0d-d56a-a56b9deb8455"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model berhasil dikonversi dan disimpan sebagai model_quant_float16.tflite\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}